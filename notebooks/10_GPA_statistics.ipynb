{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPA statistics",
   "id": "79ef7d18edda9723"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load `mean_shape`, model and dataset",
   "id": "3bf77a9187c097d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T04:31:00.327895Z",
     "start_time": "2025-10-28T04:30:48.825785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from wings.config import PROCESSED_DATA_DIR, MODELS_DIR\n",
    "from wings.modeling.litnet import LitNet\n",
    "from wings.modeling.loss import DiceLoss\n",
    "\n",
    "mean_coords = torch.load(\n",
    "    PROCESSED_DATA_DIR / \"mask_datasets\" / 'rectangle' / \"mean_shape.pth\", weights_only=False\n",
    ")\n",
    "\n",
    "checkpoint_path = MODELS_DIR / 'unet-rectangle-epoch=08-val_loss=0.14-unet-training-rectangle_1.ckpt'\n",
    "unet_model = torch.hub.load(\n",
    "    'mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=False\n",
    ")\n",
    "num_epochs = 60\n",
    "model = LitNet.load_from_checkpoint(checkpoint_path, model=unet_model, num_epochs=num_epochs, criterion=DiceLoss())\n",
    "model.eval()\n",
    "\n",
    "test_dataset = torch.load(\n",
    "    PROCESSED_DATA_DIR / \"mask_datasets\" / 'rectangle' / \"test_mask_dataset.pth\",\n",
    "    weights_only=False\n",
    ")\n",
    "max_n = len(test_dataset)\n"
   ],
   "id": "6ed62a3380141ed6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-10-28 12:30:52.324\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mwings.config\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m40\u001B[0m - \u001B[1mPROJ_ROOT path is: /home/mkrajew/bees\u001B[0m\n",
      "\u001B[32m2025-10-28 12:30:52.591\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mwings.config\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m62\u001B[0m - \u001B[1mtorch.cuda.get_device_name()='NVIDIA RTX A3000 12GB Laptop GPU'\u001B[0m\n",
      "Using cache found in /home/mkrajew/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocess data",
   "id": "c5dfb916ae3a5c67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T04:35:29.626315Z",
     "start_time": "2025-10-28T04:34:59.489824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from wings.gpa import recover_order, center_shape, normalize_shape, procrustes_align\n",
    "from wings.visualizing.image_preprocess import final_coords\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "original_labels = []\n",
    "predicted_labels = []\n",
    "bad_masks = 0\n",
    "bad_indices = []\n",
    "\n",
    "gpas = []\n",
    "\n",
    "for idx, (image, _, coords, (x_size, y_size)) in enumerate(tqdm(test_dataset, desc=\"Evaluating\")):\n",
    "    image = image.to(device).unsqueeze(0)\n",
    "\n",
    "    try:\n",
    "        output = model(image)\n",
    "        mask = torch.round(output).squeeze().detach().cpu().numpy()\n",
    "\n",
    "        mask_coords = final_coords(mask, x_size, y_size)\n",
    "\n",
    "        reordered = recover_order(mean_coords, torch.tensor(mask_coords), device=device)\n",
    "        orig = coords.view(-1, 2)\n",
    "\n",
    "        gpa = procrustes_align(normalize_shape(center_shape(reordered)), mean_coords)\n",
    "        gpas.append(gpa.cpu().numpy())\n",
    "\n",
    "        original_labels.append(orig.cpu().numpy())\n",
    "        predicted_labels.append(reordered.cpu().numpy())\n",
    "\n",
    "    except Exception as e:\n",
    "        bad_masks += 1\n",
    "        bad_indices.append(idx)\n",
    "        continue\n",
    "\n",
    "print(f\"Total samples: {len(test_dataset)}\")\n",
    "print(f\"Failed masks: {bad_masks}\")\n",
    "\n",
    "original_labels = np.stack(original_labels)\n",
    "predicted_labels = np.stack(predicted_labels)\n",
    "gpas = np.stack(gpas)\n",
    "\n",
    "print(f\"original_labels.shape = {original_labels.shape}\")\n",
    "print(f\"predicted_labels.shape = {predicted_labels.shape}\")\n"
   ],
   "id": "5f29a118ac725783",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2172/2172 [00:30<00:00, 72.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2172\n",
      "Failed masks: 37\n",
      "original_labels.shape = (2135, 19, 2)\n",
      "predicted_labels.shape = (2135, 19, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate Statistics",
   "id": "88d11e91745c7ee0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "errors = np.linalg.norm(predicted_labels - original_labels, axis=2)  # shape: (n_samples, 19)\n",
    "print(errors.size)\n",
    "\n",
    "gpa_errors = np.linalg.norm(mean_coords.cpu().numpy() - gpas, axis=2)\n",
    "gpa_errors.shape\n"
   ],
   "id": "3b6b4d3e4209015f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Mean Error",
   "id": "9062c896d5af9a98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Per point",
   "id": "29fba7b16e63bc4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "mean = errors.mean()\n",
    "median = np.median(errors)\n",
    "\n",
    "mean_error = errors.mean(axis=0)\n",
    "min_error = errors.min(axis=0)\n",
    "max_error = errors.max(axis=0)\n",
    "\n",
    "rmse_error = np.sqrt(np.mean(errors ** 2, axis=0))\n",
    "median_error = np.median(errors, axis=0)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"MAE\": mean_error,\n",
    "        \"RMSE\": rmse_error,\n",
    "        \"Median\": median_error,\n",
    "        \"Min\": min_error,\n",
    "        \"Max\": max_error,\n",
    "    }, index=np.arange(1, 20)\n",
    ")\n",
    "\n",
    "print(f\"{mean=}\")\n",
    "print(f\"{median=}\\n\")\n",
    "\n",
    "df.index.name = \"Point\"\n",
    "df = df.round(3)\n",
    "\n",
    "print(df)\n"
   ],
   "id": "d1e024d671ddf410",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Per image",
   "id": "d26bc9dfdfe4d683"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean_per_image = errors.mean(axis=1)\n",
    "# mean_per_image = np.sqrt(np.mean(errors**2, axis=1))\n",
    "\n",
    "gpa_error_per_image = gpa_errors.mean(axis=1)\n",
    "\n",
    "mae = np.mean(np.abs(mean_per_image))\n",
    "rmse = np.sqrt(np.mean(mean_per_image ** 2))\n",
    "median_error = np.median(mean_per_image)\n",
    "min_error = np.min(mean_per_image)\n",
    "max_error = np.max(mean_per_image)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"MAE\": [mae],\n",
    "        \"RMSE\": [rmse],\n",
    "        \"Median\": [median_error],\n",
    "        \"Min\": [min_error],\n",
    "        \"Max\": [max_error]\n",
    "    }\n",
    ").round(3)\n",
    "\n",
    "print(df)\n"
   ],
   "id": "b0dffb6bb2f8941e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_error_per_image = np.max(errors, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(mean_per_image, gpa_error_per_image)\n",
    "\n",
    "plt.title(\"Max vs GPA Error\")\n",
    "plt.xlabel(\"Max error per Image\")\n",
    "plt.ylabel(\"GPA Error per Image\")\n",
    "plt.grid(True)\n",
    "# plt.xlim(0, 25)\n",
    "plt.show()\n"
   ],
   "id": "c6aeb97471aab88e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Images with the worst statistics",
   "id": "bd4da02aca844037"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Single point",
   "id": "b60f46c71c0e54dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 10\n",
    "\n",
    "flat_errors = errors.flatten()\n",
    "\n",
    "# Get indices of top-n largest errors\n",
    "top_n_flat_indices = np.argpartition(-flat_errors, n)[:n]\n",
    "top_n_flat_indices = top_n_flat_indices[np.argsort(-flat_errors[top_n_flat_indices])]\n",
    "\n",
    "# Convert back to 2D indices (sample index, inner index)\n",
    "top_n_indices_2d = np.array(np.unravel_index(top_n_flat_indices, errors.shape)).T  # shape: (n, 2)\n",
    "top_n_sample_indices = top_n_indices_2d[:, 0]\n",
    "\n",
    "all_indices = list(range(len(test_dataset)))\n",
    "good_indices = [i for i in all_indices if i not in bad_indices]\n",
    "\n",
    "original_dataset_indices_per_point = [good_indices[i] for i in top_n_sample_indices]\n",
    "\n",
    "print(\"Top-n errors (dataset_index, point, error_value):\")\n",
    "for idx, ((sample_idx, inner_idx), dataset_idx) in enumerate(zip(top_n_indices_2d, original_dataset_indices_per_point)):\n",
    "    print(\n",
    "        f\"{idx + 1:2.0f}:\\tIndex={dataset_idx:5.0f}\\t\\tPoint {inner_idx:2.0f}\\tError={errors[sample_idx, inner_idx]:7.2f}\"\n",
    "    )\n",
    "\n",
    "print(original_dataset_indices_per_point)\n"
   ],
   "id": "1ccafe0feaa58828",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Whole image mean",
   "id": "f9d3775887b507f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 10\n",
    "\n",
    "mean_errors_per_image = np.mean(errors, axis=1)  # shape: (n_images,)\n",
    "\n",
    "top_n_indices = np.argpartition(-mean_errors_per_image, n)[:n]\n",
    "top_n_indices = top_n_indices[np.argsort(-mean_errors_per_image[top_n_indices])]\n",
    "\n",
    "all_indices = list(range(len(test_dataset)))\n",
    "good_indices = [i for i in all_indices if i not in bad_indices]\n",
    "original_dataset_indices_per_image = [good_indices[i] for i in top_n_indices]\n",
    "\n",
    "print(\"Top-n images with largest mean error (dataset_index, mean_error):\")\n",
    "for rank, (sample_idx, dataset_idx) in enumerate(zip(top_n_indices, original_dataset_indices_per_image), start=1):\n",
    "    print(f\"{rank:2}: Index={dataset_idx:5}\\t\\tMean error={mean_errors_per_image[sample_idx]:7.3f}\")\n"
   ],
   "id": "ec1849fa8e906bad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Threshold\n",
    "\n",
    "How many points are there, which have error greater than threshold and how many images are there that mean error on every point is greater than threshold"
   ],
   "id": "958ca48e54fe3935"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "threshold = 3.5\n",
    "\n",
    "num_errors_above_threshold = np.sum(errors > threshold)\n",
    "total_points = errors.size\n",
    "percentage = (num_errors_above_threshold / total_points) * 100\n",
    "\n",
    "print(f\"Number of points with error > {threshold}: {num_errors_above_threshold}\")\n",
    "print(f\"Percent of all points: {percentage:.2f}%\")\n"
   ],
   "id": "7651c8f97bcbdbd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Threshold graph",
   "id": "b080cfc23ae28773"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "flat_errors = errors.flatten()\n",
    "# m = int(np.ceil(flat_errors.max()))\n",
    "m = 8\n",
    "\n",
    "x_values = np.arange(0, m + 1)\n",
    "\n",
    "counts = [np.sum(flat_errors > x) for x in x_values]\n",
    "percentages = [100 * c / len(flat_errors) for c in counts]\n",
    "\n",
    "# Graph\n",
    "fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "ax1.bar(x_values, counts, color='steelblue', alpha=0.7, label='Number of points (error > x)')\n",
    "ax1.set_xlabel('Error threshold')\n",
    "ax1.set_ylabel('Number of points')\n",
    "# ax1.set_yscale('log')  # logaritmic scale\n",
    "ax1.grid(True, which=\"both\", axis='y', linestyle='--', alpha=0.5)\n",
    "ax1.set_title('Number and percentage of points with error greater than x')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x_values, percentages, color='darkorange', marker='o', label='Percentage of points (%)')\n",
    "ax2.set_ylabel('Percentage of all points')\n",
    "\n",
    "# ax1.set_ylim(bottom=0)\n",
    "# ax2.set_ylim(bottom=0)\n",
    "\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ef26adb8475b3933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Threshold graph per image",
   "id": "e87069eda5862528"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_errors_per_image = np.mean(errors, axis=1)\n",
    "\n",
    "m = 8\n",
    "x_values = np.arange(1, m + 1)\n",
    "\n",
    "counts = [np.sum(mean_errors_per_image > x) for x in x_values]\n",
    "percentages = [100 * c / len(mean_errors_per_image) for c in counts]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "ax1.bar(x_values, counts, color='steelblue', alpha=0.7, label='Number of images (mean error > x)')\n",
    "ax1.set_xlabel('Error threshold')\n",
    "ax1.set_ylabel('Number of images')\n",
    "ax1.grid(True, which=\"both\", axis='y', linestyle='--', alpha=0.5)\n",
    "ax1.set_title('Number and percentage of images with mean error greater than x')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x_values, percentages, color='darkorange', marker='o', label='Percentage of images (%)')\n",
    "ax2.set_ylabel('Percentage of all images')\n",
    "\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "db3496f9cc2435d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "threshold = 3\n",
    "\n",
    "mean_errors_per_image = np.mean(errors, axis=1)\n",
    "\n",
    "num_images_above_threshold = np.sum(mean_errors_per_image > threshold)\n",
    "total_images = len(mean_errors_per_image)\n",
    "percentage = (num_images_above_threshold / total_images) * 100\n",
    "\n",
    "print(f\"Number of images with mean error > {threshold}: {num_images_above_threshold}\")\n",
    "print(f\"Percent of all images: {percentage:.2f}%\")\n"
   ],
   "id": "3c8510ff682123ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PLOT image with predicted and original coordinates",
   "id": "9377d4e7d90c7273"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "values = original_dataset_indices_per_image\n",
    "values_iter = iter(values)\n"
   ],
   "id": "827b957f5749c768",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from wings.visualizing.visualize import visualize_coords\n",
    "from wings.visualizing.image_preprocess import final_coords\n",
    "import cv2\n",
    "from wings.config import RAW_DATA_DIR, IMG_FOLDER_SUFX\n",
    "\n",
    "# n = np.random.randint(0, max_n)\n",
    "# n = 522\n",
    "try:\n",
    "    n = next(values_iter)\n",
    "except StopIteration:\n",
    "    values_iter = iter(values)\n",
    "    n = next(values_iter)\n",
    "\n",
    "image, _, coords, (x_size, y_size) = test_dataset[n]\n",
    "original_dataset = test_dataset.dataset  # this gives you the original dataset\n",
    "filename = original_dataset.coords_df.loc[test_dataset.indices[n], 'file']\n",
    "\n",
    "country = filename.split('-', 1)[0]\n",
    "imgpath = RAW_DATA_DIR / f\"{country}{IMG_FOLDER_SUFX}\" / filename\n",
    "img = cv2.imread(imgpath, cv2.IMREAD_COLOR)\n",
    "\n",
    "try:\n",
    "    output = model(image.cuda().unsqueeze(0))\n",
    "    mask = torch.round(output).squeeze().detach().cpu().numpy()\n",
    "\n",
    "    mask_coords = final_coords(mask, x_size, y_size)\n",
    "\n",
    "    flat_coords = [coord for pair in mask_coords for coord in pair]  # Flatten list of tuples\n",
    "    target = torch.tensor(flat_coords, dtype=torch.float32)\n",
    "\n",
    "    # Visualize\n",
    "    spot_size = 3\n",
    "    img = visualize_coords(img, coords, spot_size=spot_size, color=(255, 0, 0), show=False)\n",
    "    print(n)\n",
    "    img = visualize_coords(img, target, spot_size=spot_size, color=(0, 255, 0), filename=filename)\n",
    "except Exception as e:\n",
    "    plt.imshow(mask)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(e)\n"
   ],
   "id": "c5f2aacb6db96a7a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
